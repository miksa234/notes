\include{./preamble.tex}


\begin{document}

\maketitle

\tableofcontents

\section{Intro}
The following questions are answered:
\begin{itemize}
    \item iterative regularization with NN functions
    \item application of NNs on inverse problems
    \item What generalized NNs are best suited for IPs?
\end{itemize}

\subsection{Posing the problem}
Consider linear operator equation between Hilbert spaces $\mathbf{X}$ and
$\mathbf{Y}$
\begin{align}
    F\mathbf{x} = \mathbf{y}.
\end{align}
For the problem modeling we introduce a function, called \textbf{Coding}
$\Psi: \vec{P} \to \mathbf{X}$ which maps NN parameters to images functions.
Our problem can be written as follows
\begin{align}
    N(\vec{p}) = F\Psi(\vec{p}) = \mathbf{y},
\end{align}
where $X$ is the image space, $Y$ the data space and $\vec{P}$ the parameter
space. In the case the operator in question $F$ is nonlinear then we would of
course have a nonlinear equation, which we are not considering right now.

\subsection{Shallow neural network coders}
Shallow neural network coders are of the following form
\begin{align}
    \Psi:
    \mathcal{D}(\Psi) := \mathbb{R}^{n_*} =
    \mathbb{R}^{N}\times \mathbb{R}^{n \times N}
    \times \mathbb{R}^{N}
    &\to \mathbf{X} :=
    L^{2}\left([0, 1]^{n}\right),\\
    \vec{p} = (\vec{\alpha}, \mathbf{w}, \vec{\theta}) &\mapsto
    \left(\vec{x} \to \sum_{j=1}^{N} \alpha_j\sigma\left(
    \vec{\mathbf{w}}_j^{T}\vec{x} + \omega_j \right)  \right),
\end{align}q
where $\sigma$ is an activation function, such as tanh or sigmoid.

\subsection{Deep neural networks}
A standard DNN with $L$ layers is a function depending on $\vec{x} \in
\mathbb{R}^{n}$ with parameters $\vec{p}:=\left( \vec{\alpha}_l,
\mathbf{w}_l, \vec{\theta}_l  \right)_{l=1}^{L}$
\begin{align}
    \vec{x}\to\Psi(\vec{x}) := \sum_{j_L=1}^{N_L} \alpha_{j_L,L}\sigma_L\
    \left( p_{j_L, L} \left( \sum_{j_{L-1}=1}^{N_{L-1}}\cdots
    \left( \sum_{j_1=1}^{N_1}\alpha_{j_1,1}\sigma_1\left(p_{j_1,1}(\vec{x})
    \right)  \right)  \right)  \right),
\end{align}
where
\begin{align}
    p_{j_l}(\vec{x}) = \mathbf{w}_{j, l}^{T}\vec{x} + \theta_{j,l},
\end{align}
with $\alpha_{j,l}, \theta_{j,l} \in \mathbb{R}$ and $\vec{x},
\mathbf{w}_{j,l} \in \mathbb{R}^{n} \;\; \forall l=1,\ldots,L$. And is
probably not a Lipschitz-continuous immersion!


\section{Solution}
The solution involves either reconstructing the function or the coefficient use
Tikhonov regularization( TODO: Tikhonov regularization introduction! ) or use
newton type methods.

Using variational methods, Tikhonov regularization (some background on this
here)
\begin{align}
    \|N(\vec{p}) - \mathbf{y}\|^{2} + \alpha \|\vec{p}\|^{2} \to \min,
\end{align}
or alternatively state space regularization (some background on this)
\begin{align}
    \|N(\vec{p}) - \mathbf{y}\|^{2}
    + \alpha \|\mathbf{x} - \mathbf{x}_0\|^{2}
    \to \min \quad \text{s.t} \quad \Psi(\vec{p}) = \mathbf{x}.
\end{align}
Alternatively use iterative methods, Newton's iteration would look like the
following
\begin{align}
    \vec{p}\;^{k+1} = \vec{p} - N'\left(p^{-k}\right)^{-1}\left(N(\vec{p}) -
    \mathbf{y}  \right),
\end{align}
where $N'$ is the Jacobian.
\subsection{Decomposition cases (review)}
The \textit{1st decomposition case}
\begin{align}
    N(\vec{p}) = \Psi(F\vec{p}).
\end{align}
The \textit{2nd decomposition case}
\begin{align}
    N(\vec{p}) = F\Psi(\vec{p}).
\end{align}
Usually it is assumed that the nonlinear operator $\Psi$ is well-posed.
Here we need to see B. Hofmann On the degree of ill-posedness of nonlinear
problems.
\subsection{Gauss-Newton type method for 2nd decomposition case}
We are dealing with the operator $\Psi:\mathcal{D} \subseteq \vec{P} :=
\mathbb{R}^{n_*} \to \mathbf{X}$. The derivative of $\Psi$ \textbf{cannot be
invertible}!. So how do we decompose the 2nd case
\begin{align}
    N(\vec{p}) = F\Psi(\vec{p}).
\end{align}
To answer this we introduce the Lipschitz-differentiable immersion
definition.
\begin{mydef}
    Let there be $n_* = N*(n+2)$ neural nets depending on the parameters
    $(\vec{\alpha}, \mathbf{w}, \vec{\theta})$. Let $\Psi'$ be
    Lipschitz-continuous and
    \begin{align}
        \text{span}\{\partial_{p_i}\Psi(\vec{p})\;:\;i=1,\ldots,n_*\},
    \end{align}
    has $\text{rank}(n_*)$.
    And let $\Psi'(\vec{p})^{\dagger}$ denote a generalized inverse,
    which replaces the standard $\Psi^{-1}$ in the standard Newton's method.
\end{mydef}
\subsection{Local convergence of Ga.uss-Newton's method}
We can prove under condition that we can attain the data, i.e. reconstruct
the coefficients.
\begin{theorem}
    Let $F: \mathbf{X} \to \mathbf{Y}$ be linear with trivial nullspace and
    dense range, $\Psi:\mathcal{D} \subseteq P \to \mathbf{X}$ be
    Lipschitz-differentiable immersion and $N = F\circ \Psi$ and
    $N(\vec{p}\;^{\dagger}) = \mathbf{y}$.
    Also let $\vec{p}\;^{0} \in \mathcal{D}(\Psi)$ be sufficiently close to
    $\vec{p}\;^{\dagger}$. Then the Gauss-Newton's iteration
    \begin{align}
        \vec{p}\;^{k+1} = \vec{p}\;^{k} - N'(\vec{p})^{\dagger}
        \left( N\left( \vec{p}\;^{k} \right) - \mathbf{y} \right)
    \end{align}
    is well-defined and converges to $\vec{p}\;^{\dagger}$
\end{theorem}
\begin{proof}
    Verification of a sort of Newton-Mysovskii conditions using that
    \begin{align}
        N'(\vec{p})^{\dagger}N'(\vec{p}) =
        \Psi(\vec{p})^{\dagger}\Psi(\vec{p}),
    \end{align}
    Here we need this Otmar mentions "Barbaras book find this out 86"
    probably Barbara Kaltenbacher has some book. Its most likely this book
    B.Kaltenbacher, A.Neubauer, and O.Scherzer.
    Iterative Regularization Methods for Nonlinear Problems.
    de Gruyter, Berlin, New York, 2008.
    Also P.Deuflhard, H.W. Engl and O. Scherzer "A convergence analysis of
    iterative methods for the solution of nonlinear ill-posed problems under
    affinely invariant conditions.
\end{proof}
Then the Gauss-Newton is quadratically convergent.
\subsection{Newton's method with the neural network operator}
Convergence is based on the immersion property of the network functions
\begin{align}
    \text{span}\{\partial_{p_i}\Psi(\vec{p})\;:\;i=1,\ldots,n_*\}, \qquad
    \text{has rank}(n_*).
\end{align}
For $\alpha_i \neq 0$, this, in particular, requires that the functions
\begin{align}
    &\rho = \sum_{i=1}^{n}w_s^{i}x_i + \theta_s \\
    & \frac{\partial \Psi}{\partial \alpha_s} =\sigma(\rho),\quad
     \frac{\partial \Psi}{\partial w_s^{t}} =\sigma'(\rho)x_t,\quad
     \frac{\partial \Psi}{\partial \theta_s} =\sigma'(\rho)
\end{align}
are \textbf{linearly independent} and that $\alpha_s \neq 0$ -
\textbf{sparse} coefficients cannot be recovered.
\subsection{Linear independence problem}
The question is if
\begin{align}
    \frac{\partial \Psi}{\partial \alpha_s} ,
    \frac{\partial \Psi}{\partial w_s^{\dagger}} ,
    \frac{\partial \Psi}{\partial \theta_s}
\end{align}
Partial answer for $\frac{\partial \Psi}{\partial \alpha_s} (\vec{x}) =
\sigma\left( \sum_{i=1}^{n} w_s^{i}x_i + \theta_s \right)$ in the
Lamperski (2022) theorem:
\begin{theorem}
    For all activation functions \textit{HardShrink, HardSigmoid, HardTanh,
    HardSwish, LeakyReLU, PReLU, ReLU, ReLU6, RReLU, SoftShring, Threshold,
    LogSigmoid, Sigmoid, SoftPlus, Tanh and TanhShring and the PyTorch
    functions CELU, ELU, SELU} the shallow neural network functions formed by
    \textbf{randomly generated vectors} $(\mathbf{w}, \vec{\theta})$ are
    \textbf{linearly independent}.
\end{theorem}
Proof in A. Lamperski 2022 "Neural Network Independence Properties with
Applications to Adaptive Control". But here we need more that the first
derivative of the sigmoid functions and the first moment of the first
derivative together with the above result are linearly independent.
But the answer is not satisfactory because its not known. More or less with a
probability $1$ we can prove that the functions above are linearly
independent

\subsection{Symmetries}
For the sigmoid function we have some odious symmetries because
\begin{align}
    \sigma'(\mathbf{w}^{T}_j \vec{x} + \theta_j)
    = \sigma'\left(-\mathbf{w}_j^{T}\vec{x} - \theta_j  \right)
\end{align}
or in another formulation
\begin{align}
    \Psi'(\vec{\alpha}, \mathbf{w}, \vec{\theta}) = \Psi'(\vec{\alpha},
        -\mathbf{w}, -\vec{\theta})
\end{align}
Conjecture: ovoious symmetries = "random set" from Lamperski 2022. The
summery of the theorem
\begin{theorem}
    Assume that the activation functions are locally linearly independent.
    Then the Gauss-Newton method is converging.
\end{theorem}
\section{Results}
\subsection{Numerical results(simplified)}
The simplification is
\begin{align}
    &N = F \circ \Psi \\
    &\mathbf{y}^{\dagger} = F\Psi(\vec{p}\;^{\dagger}) \qquad \text{is
    attainable}
\end{align}
Then the Gauss-Newton method is
\begin{align}
    \vec{p}\;^{k+1} = \vec{p}\;^{k} - \Psi'\left(\vec{p})\;^{k}  \right)^{\dagger}
    \left( \Psi(\vec{p}\;^{k} - \Psi^{\dagger} \right) \qquad k \in
    \mathbb{N}_0.
\end{align}
Do some numerical  results or explain the ones in the talk.
\subsection{Landweber iteration}
Instead of the Gauss-Newton iteration we consider the Landweber iteration
\begin{align}
    \vec{p}\;^{k+1} = \vec{p}\;^{k} - \lambda \Psi'\left(\vec{p}\;^{k})  \right)^{\dagger}
    \left( \Psi(\vec{p}\;^{k} - \Psi^{\dagger} \right) \qquad k \in
    \mathbb{N}_0.
\end{align}
Needs about 500 iterations
\subsection{The catch}
If the observed convergence rate of the Gauss-New ton change completely if the
solution is not attainable. Then the conjecture is that the non-convergence
because of multiple solutions.
Also the implementation of the simplified Gauss-Newton requires inversion of
$F$ , which is not done in practice, this is for Landweber.

\subsection{Alternative to DNNs}
Instead of using Deep Neural Networks where we do not know the result if the
the immersion is invertible, we consider Quadratic neural network functions
defined as follows
\begin{align}
    \Psi(\vec{x}) := \sum_{j=1}^{N} \alpha_j\sigma\left(\vec{x}^{T}A_j\vec{x}
        + \mathbf{w}_j^{T}\vec{x} + \theta_j \right),
\end{align}
with $\alpha_j, \theta_j \in \mathbb{R}, \mathbf{w}_j \in \mathbb{R}^{n}$
and $A_j \in \mathbb{R}^{n \times n}$. We can also constrain the class of
$A_j$ and $\mathbf{w}_j$ which leads us to circular networks, circular
affine, elliptic, parabolic...
\begin{theorem}
    Quadratic neural network functions satisfy the universal approximation
    property.
\end{theorem}
The immersion property of circular network functions
\begin{align}
    \Psi(\vec{x}) := \sum_{j=1}^{N} \alpha_j\sigma\left(r_j\vec{x}^{T}\vec{x}
        + \mathbf{w}_j^{T}\vec{x} + \theta_j \right),
\end{align}
and
\begin{align}
    \text{span}\{\partial_{p_i}\Psi(\vec{p})\;:\;i=1,\ldots,n_*\}, \qquad
    \text{has rank}(n_*).
\end{align}
For $\alpha_i \neq 0$, this in particular requires that the functions
\begin{align}
    \frac{\partial \Psi}{\partial r_s}  = \sigma\left( \rho \right)
    \vec{x}^{T}\vec{x}, \quad
    \frac{\partial \Psi}{\partial \alpha_s}  = \sigma\left( \rho \right)
    , \quad
    \frac{\partial \Psi}{\partial w_s^{t}}  = \sigma'\left( \rho \right)
    x_t,\quad
    \frac{\partial \Psi}{\partial \theta_s}  = \sigma'\left( \rho \right)
\end{align}
are \textbf{linearly independent}.
\newline

Results of these types of networks is that the Shepp-Logan can be represented
with \textbf{10 nodes} with elliptic neurons and \textbf{one layer}. Where as
for affine networks, both shallow and deep we need infinity neurons. Here
figure tensorflow approximations of a circle 15 neurons linear.



%\printbibliography

\end{document}
