\include{./preamble.tex}


\begin{document}

\maketitle

\tableofcontents

\section{Intro}
The following questions are answered:
\begin{itemize}
    \item iterative regularization with NN functions
    \item application of NNs on inverse problems
    \item What generalized NNs are best suited for IPs?
\end{itemize}

\subsection{Posing the problem}
Consider linear operator equation between Hilbert spaces $\mathbf{X}$ and
$\mathbf{Y}$
\begin{align}
    F\mathbf{x} = \mathbf{y}.
\end{align}
For the problem modeling we introduce a function, called \textbf{Coding}
$\Psi: \vec{P} \to \mathbf{X}$ which maps NN parameters to images functions,
a nonlinear operator. Our problem can be written as follows
\begin{align}
    N(\vec{p}) = F\Psi(\vec{p}) = \mathbf{y}, \label{eq: main}
\end{align}
where $\mathbf{X}$ is the image space, $\mathbf{Y}$ the data space and $\vec{P}$ the parameter
space. In the case the operator in question $F$ is nonlinear then we would of
course have a nonlinear equation, which we are not considering right now. The
talk aims to explain the link between the general regularization of the
degree of ill-posedness and nonlinearity and investigates generalized
Gauss-Newton solvers, by the outer inverse or by approximations.
\subsection{Decomposition cases (review)}
An operator $N$ satisfies the \textit{1st decomposition case} in an open empty
neighborhood $\mathcal{B}\left(\vec{p}\;^{\dagger}; \rho \right) \subseteq
\vec{P} $ (an open ball at point $\vec{p}\;^{\dagger}$ with radius $\rho$), if
there exists a linear operator $F:\vec{P}\to \mathbf{X}$ and a nonlinear
operator $\Psi:\mathbf{X} \to \mathbf{Y}$ such that.
\begin{align}
    N(\vec{p}) = \Psi(F\vec{p}).
\end{align}
The \textit{2nd decomposition case} for operator $N$ in the same setting is
satisfied, if there exists a linear operator $F: \mathbf{X} \to \mathbf{Y}$
and a nonlinear operator $\Psi: \vec{P} \to \mathbf{X}$ such that
\begin{align}
    N(\vec{p}) = F\Psi(\vec{p}).
\end{align}
\subsection{Gauss-Newton type method for 2nd decomposition case}
We are dealing with the operator $\Psi:\mathcal{D} \subseteq \vec{P} :=
\mathbb{R}^{n_*} \to \mathbf{X}$. The derivative of $\Psi$ \textbf{cannot be
invertible}!. So how do we decompose the 2nd case
\begin{align}
    N(\vec{p}) = F\Psi(\vec{p}).
\end{align}
To prove convergence we need introduce the Lipschitz-differentiable immersion.
\section{Background}
\subsection{Newton-Mysovskii}
The local convergence of the Newton method is guaranteed under the so called Newton-Mysovskii
 conditions. In this section the results are shown for the simple case in the
 finite dimensional space, when the nonlinear operator has derivative which
 are invertible. This result is going to be extended as aim of the summary.

 \begin{theorem}
     Let $N: \mathcal{D}(N) \subseteq \mathbb{R}^{n}\to \mathbb{R}^{n}$ be
     continuously Fr\'echet differentiable on a non-empty, open and convex
     set $\mathcal{D}\left( N \right) $. Let $\vec{p}\;^{\dagger} \in
     \mathcal{D}(N)$ be the solution of $N(\vec{p}\;) = \mathbf{y}$, where
     $\mathbf{y} \in \mathbb{R}^{n}$. Also assume that
     \begin{enumerate}
         \item $N'(\vec{p}\;)$ is invertible $\forall \vec{p} \in
             \mathcal{D}(n)$ and
         \item The Newton-Mysovskii condition hold, i.e. $\exists C_N > 0:$
            \begin{align}
                &\big\| N'(\vec{p}\;)^{-1}\left( N'(\vec{q} + s(\vec{p} -
                \vec{q}\;) - N'(\vec{q}\;)\right) (\vec{p} - \vec{q})
                \big\|_{\vec{P}}
                \leq s C_N \|\vec{p} - \vec{q} \;\|^{2}_{\vec{P}}\\
                & \forall \vec{p}, \vec{q} \in \mathcal{D}(N), \; s \in[0,
                1],
            \end{align}
     \end{enumerate}
     Now let $\vec{p}\;^{0} \in \mathcal{D}(N)$ the starting point of the
     Newton iteration, which should be sufficiently close to the solution,
     i.e. satisfying
     \begin{align}
         &\overline{\mathcal{B}\left(\vec{p}\;^{0}, \rho \right)}\subseteq
         \mathcal{D}(N) \qquad \text{with}\\
         &\rho := \|\vec{p}\;^{\dagger} - \vec{p}^{0}\|_{\vec{P}} \quad
         \text{and} \quad h:= \frac{\rho C_l C_L}{2} <1. \label{eq: locality}
     \end{align}
     Then the Newton iteration with starting point $\vec{p}\;^{0}$ and
     iterates $\left\{\vec{p}\;^{k}  \right\}_{k \in \mathbb{N}_0} \subseteq
     \overline{\mathcal{B}(\vec{p}\;^{0}, \rho)} $ of the
     form
     \begin{align}
         \vec{p}\;^{k+1} = \vec{p}\;^{k} - N'(\vec{p}\;^{k})^{-1}\left(
         N\left(\vec{p}\;^{k}  \right) - \mathbf{y} \right)  \qquad k \in
         \mathbb{N}_0,
     \end{align}
     converge to $\vec{p}\;^{\dagger} \in \overline{\mathcal{B}(\vec{p}\;^{0},
     \rho)}$, \textbf{quadratically}.
 \end{theorem}

\subsection{Moore-Penrose Inverse}
We study the case where $\mathbf{Y}$ is an infinite dimensional Hilbert
space. In this regard it is necessary to replace the inverse in the classical
Newton method because the liberalizations of the operator $N$ cannot be be
invertible. This is done by introducing the so called Moore-Penrose inverse
or more general the outer inverse and we refer to the Gauss-Newton method to
distinguish between the classical version.
\begin{mydef}{Inner, outer and Moore Penrose inverse \label{def:
    moore-penrose}}
    $L: \vec{P} \to \mathbf{Y}$ be a linear and bounded operator between
    vector spaces $\vec{P}$ and $\mathbf{X}$. Then
    \begin{enumerate}
        \item $B: \mathbf{Y} \to \vec{P}$ is called \textbf{left-inverse} to
            $L$ if
            \begin{align}
                BL = I
            \end{align}
        \item $B: \mathbf{Y} \to \vec{P}$ is called \textbf{right-inverse} to
            $L$ if
            \begin{align}
                LB = I
            \end{align}
        \item $B: \vec{P} \to \vec{P}$ is called an \textbf{inverse} to
            $L$ if it is both a left and a right inverse to $L$.
        \item $B: \vec{P} \to \vec{P}$ is called an \textbf{outer inverse} to
            $L$ if
            \begin{align}
                BLB = B
            \end{align}
        \item Let $\vec{P}$ and $\mathbf{Y}$ be Hilbert spaces, $L: \vec{P}
            \to \mathbf{Y}$ be linear bounded operator. Denote the
            orthogonal projections $P$ and $Q$ onto the nullspace of $L$,
            $\mathcal{N}(L)$ closed and the closure of the range of $L$,
            $\overline{\mathcal{R}\left(L  \right)} $. This means that for all $\vec{p}
            \in \vec{P}$ and $\mathbf{y} \in \mathbf{Y}$ we have
            \begin{align}
                &P\vec{p} = \text{argmin}
                \left\{
                    \|\vec{p}_1-\vec{p}\|_{\vec{P}} : \vec{p}_1 \in
                \mathcal{N}(L) \right\},\\
                &Q\mathbf{y} = \text{argmin}
                \left\{
                    \|\mathbf{y}_1 - \mathbf{y}\|_\mathbf{Y}: \mathbf{y} \in
                    \overline{\mathcal{R}(L)} \right\}
            \end{align}
            Then the operator $B: \mathcal{D}(B) \subseteq \mathcal{Y} \to
            \vec{P}$ with $\mathcal{B}(B):= \mathcal{R} \dotplus
            \mathcal{R}^{\perp}$ is called \textbf{Moore-Penrose inverse} of
            $L$ if the following conditions(identities) hold
            \begin{align}
                &LBL = L, \nonumber\\
                &BLB = B, \nonumber\\
                &BL= I-P, \label{eq: moore-penrose}\\
                &LB = Q|_{\mathcal{D}(B)} \nonumber.
            \end{align}

    \end{enumerate}
    The left and right inverses are used in a different context. For a left
    inverse the nullspace of $L$ has to be trivial, in contrast to $B$.
    On the other hand for the right inverse the nullspace of $B$ needs to be
    trivial.


\end{mydef}

\subsection{Lipschitz-differentiable immersion}
\begin{mydef}{Lipschitz-differentiable immersion}
    Let there be $n_* = N*(n+2)$ neural nets depending on the parameters
    $(\vec{\alpha}, \mathbf{w}, \vec{\theta})$. Let $\Psi'$ be
    Lipschitz-continuous and
    \begin{align}
        \mathbf{X}_{\vec{p}} =
        \text{span}\{\partial_{p_i}\Psi(\vec{p})\;:\;i=1,\ldots,n_*\},
        \label{eq: lpdi-property}
    \end{align}
    has $\text{rank}(n_*)$.
    And let $\Psi'(\vec{p})^{\dagger}$ denote a generalized inverse,
    which replaces the standard $\Psi^{-1}$ in the standard Newton's method.
    TODO: more in detail definition.
\end{mydef}
We link the inverse of the Lipschitz Lipschitz-differentiable immersion with
the Moore-Penrose inverse together with the necessary boundary constraints
for the Gauss-Newton method
\begin{theorem}
    \begin{enumerate}
        \item The function $\Psi'(\vec{p})^{\dagger}: \mathbf{X} \to \vec{P}$
            is the Moore-Penrose inverse of $\Psi'(\vec{p})$.
        \item For all $\vec{p} \in \mathcal{D}(\Psi) \subseteq \vec{P}$
            there is a non-empty closed neighborhood where
            $\Psi'(\vec{p})^{\dagger}$ is uniformly bounded and it is
            Lipschitz-continuous, i.e.
            \begin{align}
                &\|\Psi'(\vec{p}\;)^{\dagger} -
                \Psi'(\vec{q}\;)^{\dagger}\|_{\mathbf{X}\to\vec{P}}
                \leq C_L \|\vec{p} - \vec{q}\;\|_{\vec{P}}&\\
                &\|\Psi'(\vec{p}\;)^{\dagger}
                \|_{\mathbf{X}\to\vec{P}} \leq C_l\qquad &\text{for}\;\;
                \vec{p}, \vec{q}\in \mathcal{D}(\Psi).
            \end{align}
    \item The operator $P_{\vec{p}}: \mathbf{X} \to \mathbf{X}_{\vec{p}}$ is
            bounded
    \end{enumerate}
\end{theorem}
\begin{proof}
    TODO: here proof
\end{proof}

We can now wrap the results back to the original problem of the Gauss-Newton
iteration of \ref{eq: main}
\begin{lemma}
    Let $F$ be as in \ref{eq: main} linear, bounded with trivial nullspace
    and dense range (for the Moore-Penrose inverse). Let $\Psi:
    \mathcal{D}(\Psi)\subseteq \mathbb{R}^{n_*} \to \mathbf{X}$ be a
    Lipschitz-differentiable immersion. And $N = F\circ \Psi$ \ref{eq: main}.
    Here it is important to see the immanent result that for $N$,
    $\mathcal{D}(N) = \mathcal{D}(\Psi)$, therefore $\forall \vec{p} \in
    \mathcal{D}$ the derivative of the operator $N$ has a Moore-Penrose
    inverse $N'(\vec{p}\;)^{\dagger}$, satisfying
    \begin{enumerate}
        \item Decomposition property of the Moore-Penrose inverse
            \begin{align}
                N'(\vec{p}\;)^{\dagger}\mathbf{z} =
                \Psi'(\vec{p}\;)^{\dagger}F^{-1}\mathbf{z} \qquad \forall
                \vec{p} \in \mathcal{D}(N),\; \mathbf{z} \in \mathcal{R}(F)
                \subseteq \mathbf{Y}
            \end{align}
            which means that
            \begin{align}
                &N'(\vec{p}\;)^{\dagger}N'(\vec{p}\;) = I \quad \text{on} \;\;
                \mathbb{R}^{n_*} \qquad \text{and}\\
                &N(\vec{p}\;)N'(\vec{p}\;)^{\dagger} =
                \mathcal{Q}|_{\mathcal{R}(FP_{\vec{p}})},
            \end{align}
            where $\mathcal{Q} : \mathbf{Y} \to
            \overline{\mathcal{R}(FP_{\vec{p}})}\dotplus\mathcal{R}(FP_{\vec{p}})^{\perp}$.
            So in the definition of the Moore-Penrose \ref{def:
            moore-penrose}, $P$ in \ref{eq: moore-penrose} is $P \equiv 0$.
        \item The generalized Newton-Mysovskii conditions are also satisfied
            \begin{align}
                &\big\| N'(\vec{p}\;)^{\dagger}\left( N'(\vec{q} + s(\vec{p} -
                \vec{q}\;) - N'(\vec{q}\;)\right) (\vec{p} - \vec{q})
                \big\|_{\vec{P}}
                \leq s C_l C_L \|\vec{p} - \vec{q} \;\|^{2}_{\vec{P}}\\
                & \forall \vec{p}, \vec{q} \in \mathcal{D}(N), \; s \in[0,
                1],
            \end{align}
            where $C_l, C_L$ are the Lipschitz-constants.
    \end{enumerate}
\end{lemma}
\begin{proof}
    TODO: add proof here.
\end{proof}
Bringing it all together to show the local convergence of the Gauss-Newton
method, where $N = F\circ \Psi$ is a composition of a linear bounded operator
and a Lipschitz-differentiable immersion.
\begin{theorem}
    Assume there exists a $\vec{p}\;^{\dagger} \in \mathcal{D}(\Psi)$ which
    satisfies
    \begin{align}
        N(\vec{p}\;^{\dagger}) = \mathbf{y},
    \end{align}
    and assume there exists a $\vec{p}\;^{0} \in \mathcal{D}(\Psi)$ as in
    \ref{eq: locality}, satisfying locality, Then the iterates
    $\{\vec{p}\;^{k}\}_{k\in \mathbb{N}_0}$  of the Gauss-Newton method of
    the form
     \begin{align}
         \vec{p}\;^{k+1} = \vec{p}\;^{k} - N'(\vec{p}\;^{k})^{\dagger}\left(
         N\left(\vec{p}\;^{k}  \right) - \mathbf{y} \right)  \qquad k \in
         \mathbb{N}_0,
     \end{align}
     are well-defined in $\overline{\mathcal{B}\left(\vec{p}\;^{0}, \rho
     \right) }$ and converge quadratically to $\vec{p}\;^{\dagger}$.
\end{theorem}
\begin{proof}
    TODO: proof here
\end{proof}


\subsection{Neural networks}
Shallow neural network coders are of the following form
\begin{align}
    \Psi:
    \mathcal{D}(\Psi) := \mathbb{R}^{n_*} =
    \mathbb{R}^{N}\times \mathbb{R}^{n \times N}
    \times \mathbb{R}^{N}
    &\to \mathbf{X} :=
    L^{2}\left([0, 1]^{n}\right),\\
    \vec{p} = (\vec{\alpha}, \mathbf{w}, \vec{\theta}) &\mapsto
    \left(\vec{x} \to \sum_{j=1}^{N} \alpha_j\sigma\left(
    \vec{\mathbf{w}}_j^{T}\vec{x} + \omega_j \right)  \right),
\end{align}q
where $\sigma$ is an activation function, such as tanh or sigmoid.

A standard deep neural network (DNN) with $L$ layers is a function depending on $\vec{x} \in
\mathbb{R}^{n}$ with parameters $\vec{p}:=\left( \vec{\alpha}_l,
\mathbf{w}_l, \vec{\theta}_l  \right)_{l=1}^{L}$
\begin{align}
    \vec{x}\to\Psi(\vec{x}) := \sum_{j_L=1}^{N_L} \alpha_{j_L,L}\sigma_L\
    \left( p_{j_L, L} \left( \sum_{j_{L-1}=1}^{N_{L-1}}\cdots
    \left( \sum_{j_1=1}^{N_1}\alpha_{j_1,1}\sigma_1\left(p_{j_1,1}(\vec{x})
    \right)  \right)  \right)  \right),
\end{align}
where
\begin{align}
    p_{j_l}(\vec{x}) = \mathbf{w}_{j, l}^{T}\vec{x} + \theta_{j,l},
\end{align}
with $\alpha_{j,l}, \theta_{j,l} \in \mathbb{R}$ and $\vec{x},
\mathbf{w}_{j,l} \in \mathbb{R}^{n} \;\; \forall l=1,\ldots,L$. And is
probably not a Lipschitz-continuous immersion!


The solution involves either reconstructing the function or the coefficient use
Tikhonov regularization or use newton type methods, the talk explains the
solution for decoposable operators wrt. the 2nd decomposition case for
Gauss-Newton type methods.

Using variational methods, Tikhonov regularization (some background on this
here)
\begin{align}
    \|N(\vec{p}) - \mathbf{y}\|^{2} + \alpha \|\vec{p}\|^{2} \to \min,
\end{align}
or alternatively state space regularization (some background on this)
\begin{align}
    \|N(\vec{p}) - \mathbf{y}\|^{2}
    + \alpha \|\mathbf{x} - \mathbf{x}_0\|^{2}
    \to \min \quad \text{s.t} \quad \Psi(\vec{p}) = \mathbf{x}.
\end{align}
Alternatively use iterative methods, Newton's iteration would look like the
following
\begin{align}
    \vec{p}\;^{k+1} = \vec{p} - N'\left(p^{-k}\right)^{-1}\left(N(\vec{p}) -
    \mathbf{y}  \right),
\end{align}
where $N'$ is the Jacobian.

Usually it is assumed that the nonlinear operator $\Psi$ is well-posed.
Here we need to see B. Hofmann On the degree of ill-posedness of nonlinear
problems. Where we assume that the nonlinear operator $\Psi$ is well-posed.


\section{Solution}
In this section the main results of the talk are explained. The aim is to use
the universal approximation properties of neural networks, the fact that they
can approximate continuous functions arbitrarily well, to the inverse problem
in \ref{eq: main} using the Gauss-Newton method. To ensure convergence it is
necessary to show that the considered neural network structure is a
Lipschitz-differentiable immersion. As it will be shown, a direct implication
of this is to show that the among other the activation function, its
derivative and its first moment of the derivative are linearly independent.
For this, results from \cite{lamperski_2022} are used and it is conjectured
that the statement from \ref{eq: lpdi-property} is fulfilled, meaning that
the function from $\mathbf{X}_{\vec{p}}$ are linearly independent.
%\subsection{Local convergence of Gauss-Newton's method}
%We can prove under condition that we can attain the data, i.e. reconstruct
%the coefficients.
%\begin{theorem}
%    Let $F: \mathbf{X} \to \mathbf{Y}$ be linear with trivial nullspace and
%    dense range, $\Psi:\mathcal{D} \subseteq P \to \mathbf{X}$ be
%    Lipschitz-differentiable immersion and $N = F\circ \Psi$ and
%    $N(\vec{p}\;^{\dagger}) = \mathbf{y}$.
%    Also let $\vec{p}\;^{0} \in \mathcal{D}(\Psi)$ be sufficiently close to
%    $\vec{p}\;^{\dagger}$. Then the Gauss-Newton's iteration
%    \begin{align}
%        \vec{p}\;^{k+1} = \vec{p}\;^{k} - N'(\vec{p})^{\dagger}
%        \left( N\left( \vec{p}\;^{k} \right) - \mathbf{y} \right)
%    \end{align}
%    is well-defined and converges to $\vec{p}\;^{\dagger}$
%\end{theorem}
%\begin{proof}
%    Verification of a sort of Newton-Mysovskii conditions using that
%    \begin{align}
%        N'(\vec{p})^{\dagger}N'(\vec{p}) =
%        \Psi(\vec{p})^{\dagger}\Psi(\vec{p}),
%    \end{align}
%    Here we need this Otmar mentions "Barbaras book find this out 86"
%    probably Barbara Kaltenbacher has some book. Its most likely this book
%    B.Kaltenbacher, A.Neubauer, and O.Scherzer.
%    Iterative Regularization Methods for Nonlinear Problems.
%    de Gruyter, Berlin, New York, 2008.
%    Also P.Deuflhard, H.W. Engl and O. Scherzer "A convergence analysis of
%    iterative methods for the solution of nonlinear ill-posed problems under
%    affinely invariant conditions.
%\end{proof}
%Then the Gauss-Newton is quadratically convergent.
\subsection{Newton's method with the neural network operator}
Convergence is based on the immersion property of the network functions
\begin{align}
    \text{span}\{\partial_{p_i}\Psi(\vec{p})\;:\;i=1,\ldots,n_*\}, \qquad
    \text{has rank}(n_*).
\end{align}
For $\alpha_i \neq 0$, this, in particular, requires that the functions
\begin{align}
    & \frac{\partial \Psi}{\partial \alpha_s} =\sigma(\rho),\quad
     \frac{\partial \Psi}{\partial w_s^{t}} =\sigma'(\rho)x_t,\quad
     \frac{\partial \Psi}{\partial \theta_s} =\sigma'(\rho),\\
    & \text{where} \qquad
    \rho = \sum_{i=1}^{n}w_s^{i}x_i + \theta_s \\
\end{align}
are \textbf{linearly independent} and that $\alpha_s \neq 0$ -
\textbf{sparse} coefficients cannot be recovered.
\subsection{Linear independence problem}
The question is if
\begin{align}
    \frac{\partial \Psi}{\partial \alpha_s} ,
    \frac{\partial \Psi}{\partial w_s^{\dagger}} ,
    \frac{\partial \Psi}{\partial \theta_s}
\end{align}
Partial answer for $\frac{\partial \Psi}{\partial \alpha_s} (\vec{x}) =
\sigma\left( \sum_{i=1}^{n} w_s^{i}x_i + \theta_s \right)$ in the
\cite{lamperski_2022} theorem:
\begin{theorem}
    For all activation functions \textit{HardShrink, HardSigmoid, HardTanh,
    HardSwish, LeakyReLU, PReLU, ReLU, ReLU6, RReLU, SoftShring, Threshold,
    LogSigmoid, Sigmoid, SoftPlus, Tanh and TanhShring and the PyTorch
    functions CELU, ELU, SELU} the shallow neural network functions formed by
    \textbf{randomly generated vectors} $(\mathbf{w}, \vec{\theta})$ are
    \textbf{linearly independent}.
\end{theorem}
Proof in A. Lamperski 2022 "Neural Network Independence Properties with
Applications to Adaptive Control". But here we need more that the first
derivative of the sigmoid functions and the first moment of the first
derivative together with the above result are linearly independent.
But the answer is not satisfactory because its not known. More or less with a
probability $1$ we can prove that the functions above are linearly
independent.

\subsection{Symmetries}
For the sigmoid function we have some odious symmetries because
\begin{align}
    \sigma'(\mathbf{w}^{T}_j \vec{x} + \theta_j)
    = \sigma'\left(-\mathbf{w}_j^{T}\vec{x} - \theta_j  \right)
\end{align}
or in another formulation
\begin{align}
    \Psi'(\vec{\alpha}, \mathbf{w}, \vec{\theta}) = \Psi'(\vec{\alpha},
        -\mathbf{w}, -\vec{\theta})
\end{align}
Conjecture: ovoious symmetries = "random set" from Lamperski 2022. The
summery of the theorem
\begin{theorem}
    Assume that the activation functions are locally linearly independent.
    Then the Gauss-Newton method is converging.
\end{theorem}
\section{Results}
\subsection{Numerical results(simplified)}
The simplification is
\begin{align}
    &N = F \circ \Psi \\
    &\mathbf{y}^{\dagger} = F\Psi(\vec{p}\;^{\dagger}) \qquad \text{is
    attainable}
\end{align}
Then the Gauss-Newton method is
\begin{align}
    \vec{p}\;^{k+1} = \vec{p}\;^{k} - \Psi'\left(\vec{p})\;^{k}  \right)^{\dagger}
    \left( \Psi(\vec{p}\;^{k} - \Psi^{\dagger} \right) \qquad k \in
    \mathbb{N}_0.
\end{align}
Do some numerical  results or explain the ones in the talk.
\subsection{Landweber iteration}
Instead of the Gauss-Newton iteration we consider the Landweber iteration
\begin{align}
    \vec{p}\;^{k+1} = \vec{p}\;^{k} - \lambda \Psi'\left(\vec{p}\;^{k})  \right)^{\dagger}
    \left( \Psi(\vec{p}\;^{k} - \Psi^{\dagger} \right) \qquad k \in
    \mathbb{N}_0.
\end{align}
Needs about 500 iterations
\subsection{The catch}
If the observed convergence rate of the Gauss-New ton change completely if the
solution is not attainable. Then the conjecture is that the non-convergence
because of multiple solutions.
Also the implementation of the simplified Gauss-Newton requires inversion of
$F$ , which is not done in practice, this is for Landweber.

\subsection{Alternative to DNNs}
Instead of using Deep Neural Networks where we do not know the result if the
the immersion is invertible, we consider Quadratic neural network functions
defined as follows
\begin{align}
    \Psi(\vec{x}) := \sum_{j=1}^{N} \alpha_j\sigma\left(\vec{x}^{T}A_j\vec{x}
        + \mathbf{w}_j^{T}\vec{x} + \theta_j \right),
\end{align}
with $\alpha_j, \theta_j \in \mathbb{R}, \mathbf{w}_j \in \mathbb{R}^{n}$
and $A_j \in \mathbb{R}^{n \times n}$. We can also constrain the class of
$A_j$ and $\mathbf{w}_j$ which leads us to circular networks, circular
affine, elliptic, parabolic...
\begin{theorem}
    Quadratic neural network functions satisfy the universal approximation
    property.
\end{theorem}
The immersion property of circular network functions
\begin{align}
    \Psi(\vec{x}) := \sum_{j=1}^{N} \alpha_j\sigma\left(r_j\vec{x}^{T}\vec{x}
        + \mathbf{w}_j^{T}\vec{x} + \theta_j \right),
\end{align}
and
\begin{align}
    \text{span}\{\partial_{p_i}\Psi(\vec{p})\;:\;i=1,\ldots,n_*\}, \qquad
    \text{has rank}(n_*).
\end{align}
For $\alpha_i \neq 0$, this in particular requires that the functions
\begin{align}
    \frac{\partial \Psi}{\partial r_s}  = \sigma\left( \rho \right)
    \vec{x}^{T}\vec{x}, \quad
    \frac{\partial \Psi}{\partial \alpha_s}  = \sigma\left( \rho \right)
    , \quad
    \frac{\partial \Psi}{\partial w_s^{t}}  = \sigma'\left( \rho \right)
    x_t,\quad
    \frac{\partial \Psi}{\partial \theta_s}  = \sigma'\left( \rho \right)
\end{align}
are \textbf{linearly independent}.
\newline

Results of these types of networks is that the Shepp-Logan can be represented
with \textbf{10 nodes} with elliptic neurons and \textbf{one layer}. Where as
for affine networks, both shallow and deep we need infinity neurons. Here
figure tensorflow approximations of a circle 15 neurons linear.



%\printbibliography

\end{document}
