\include{./preamble.tex}


\begin{document}

\maketitle

\tableofcontents

\section{Introduction}
Large step sizes may lead the loss to stabilize by making SGD bounce above a
valley. Showcase is done with mean square error. Consider a family of
prediction functions $\mathcal{H} := \{x \to h_\theta(x), \theta \in
\mathbb{R}^{p}\}$. The training loss wrt. input/output samples $(x_i,
y_i)_{i=1}^{n} \in \mathbb{R}^{d}\times\mathbb{R}$ is
\begin{align}
    \mathcal{L}(\theta) := \frac{1}{2n} \sum_{i=1} \left( h_\theta(x_i) -
    y_i \right)^{2}.
\end{align}
The setting $p \gg n$, i.e. the overparametrized setting is considered where
there exist many parameters $\theta^{*}$ that lead to zero loss or perfectly
interpolated the dataset. To find the minimizers of the risk function we
consider a SGD recursion with step size $\eta > 0$, with initial $\theta_0
\in \mathbb{R}^{p}$, for all $t \in \mathbb{N}$
\begin{align}
    \theta_{t+1} = \theta_t - \eta\left(h_{\theta_t}(x_{i_t})
    - y_{i_t}\right)
    \nabla_{\theta} h_{\theta_t}(x_{i_t}), \label{eq: sgd_it}
\end{align}
where $i_t \sim U(\{1,\ldots,n\})$, is a random variable following the
discrete uniform distribution over a sample of indices.<
\subsection{GD and SGD Relation}
The authors highlight the importance of gradient and noise, by explaining the
connection between the SGD dynamics and full batch GD plus a specific label
noise.

\begin{proposition}
    Let $(\theta_t)_{t\ge 0 }$ follow the SGD dynamics of \ref{eq: sgd_it},
    with the random sampling function $(i_t)_{t\ge_0}$. For $t\ge 0$ define
    the random vector $\xi_t \in \mathbb{R}^{n}$ s.t.:
    \begin{align}
        [\xi_t]_i := (h_{\theta_t}(x_i) - y_i)(1-n\mathbf{1}_{i=i_t}).
    \end{align}
    For $i \in \{1,\ldots,n\}$ and $\mathbf{1}_{A}$ is the indicator function
    of event $A$. Then $(\theta_t)_{\theta\ge )}$ follows the full batch
    gradient descent dynamics on $\mathcal{L}$ with label noise
    $(\xi_t)_{t\ge 0}$, i.e.
    \begin{align}
        \theta_{t+1} = \theta_t - \frac{\eta}{n} \sum_{i=1}^{n}
        \left( h_{\theta_t}(x_i) - y_i^{t} \right)
        \nabla_{\theta}h_{\theta_t}(x_i),
    \end{align}
    where we define the random labels $y^{t} := y + \xi_t$. Furthermore
    $\xi_t$ is a mean zero random vector with variance such that
    $\frac{1}{n(n-1)}\mathbb{E}\|\xi_t\|^{2} = 2 \mathcal{L}(\theta_t)$.
\end{proposition}
Two important things are are shown in the above Proposition.
\begin{enumerate}
    \item The noisy part at state $\theta$ always belongs to the linear space
        spanned by $\{\nabla_\theta h_\theta(x_1),\ldots, \nabla_\theta
        h_\theta(x_n)\}$.

    \item The loss can stabilize because of large step sizes, this may lead
        to constant effective scale of label noise, which is clearly
        explained thorough out the paper summary \cite{andriushchenko2023sgd}.
\end{enumerate}
\section{Dynamics behind Loss Stabilization}
For the generic quadratic loss $F(\beta) := \|X\beta - y\|^{2}$, gradient
descent converges with step size $\eta < \frac{2}{\lambda_{\text{max}}}$,
diverges for $\eta > \frac{2}{\lambda_{\text{max}}}$ and converges to a
bouncing 2-periodic dynamics for $\eta = \frac{2}{\lambda_{\text{max}}}$,
where $\lambda_{\text{max}}$ is the largest eigenvalue of the Hessian. On the
other for hand nonquadratic loss there exists an open interval of the step
sizes for witch the GD algorithm neither converges nor diverges \cite{andriushchenko2023sgd}.
Complementing this with an example were the loss stabilization occurs almost
surely in the case of SGD. A regression example with quadratic
parametrization on the one dimensional data inputs $x_i$ from a distribution
$\hat{\rho}$ and inputs generated by a linear model $y_i =
x_i\theta_{*}^{2}$. With the loss $F(\theta) := \frac{1}{4}
\mathbb{E}_{\hat{\rho}}(y - x\theta^{2})^{2}$, the std iterates with step
size $\eta>0$ follow for $ t \in \mathbb{N}$
\begin{align}
    \theta_{t+1} + \theta_t + \eta \theta_t x_{i_t}(y_{i_t} -
    x_{i_t}\theta_\text{t}^{2}).
\end{align}
W.l.o.g., consider $\theta_* = 1 $ and $\text{supp}(\hat{\rho})=[a,b] \subset
\mathbb{R}$. Then the following proposition holds

\begin{proposition}
    \label{prop: loss-stab}
    For any $\eta \in (a^{-2}, 1.25b^{-2})$ and initialization $\theta_0 \in
    (0, 1)$ for all $t>0$,
    \begin{align}
        &\delta_1 < F(\theta_t) < \delta_2 \qquad \text{a.s.}\\
        &\exists T > 0,\; \forall k > T:\quad \theta_{t+2k}< 1 <
        \theta_{t+2k+1} \qquad \text{a.s.}
    \end{align}
    Where $\delta_1, \delta_2, T>0$ are constants.
    TODO: give more accurate description of this garbage.
\end{proposition}
So if step sizes are large enough the \textbf{loss stabilizes} between level
sets $\delta_1$, $\delta _2$ and after some initial phase the iterates bounce
from one side of the \textbf{loss valley} to the other. Note the results
holds \textbf{almost surely}.

To further understand the effect of this loss stabilization the authors
assume perfect stabilization and conjecture that during the loss
stabilization, SGD is well modeled by GD with constant label noise.
\newline
Label noise dynamics have a connection with the Stochastic Differential
Equations (SDEs)
TODO: Stochastic differential equations connection to SGD.

To properly write a model for the stochastic differential equation (SDE)
dynamics, it needs to be considered that the drift needs to match the
gradient descent and the noise should have the same covariance structure. By
Proposition \ref{prop: loss-stab} the noise at step $\theta$ is spanned by
$\{\nabla_{\theta}h_{\theta}(x_1),\ldots,\nabla_{\theta}h_{\theta}(x_n)\}$
and has constant intensity corresponding to the loss stabilization at $\delta
>0$. The following SDE model is proposed
\begin{align}
    d\theta_t = -\nabla_{\theta} \mathcal{L}(\theta_t)dt + \sqrt{\eta\delta}
    \phi_{\theta_t}(X)^{T}dB_t,
\end{align}
where $(B_t)_{t\ge 0}$ is standard Brownian motion in $\mathbb{R}^{n}$ and
$\phi_{\theta}(X) := [\nabla_{\theta}h_{\theta}(x_i)^{T}]_{i=1}^{n} \in
\mathbb{R}^{n\times p}$ referred to as the Jacobian. This SDE can be
interpreted as the effective slow dynamics that dives the iterates while they
bounce rapidly in some directions at the level set $\delta$.
\subsection{Sparse Feature Learning}
It is begun with a simple example on diagonal linear networks to show a
sparsity inducing dynamics and then further disclosed to a general message
about the implicit bias prompted by the effective dynamics.
\newline
A diagonal linear network is a two-layer linear network with only diagonal
connections: the prediction function is $h_{u,v}(x) = \langle u, v \odot
x\rangle = \langle u \odot v, x\rangle$, where $\odot$ denotes the
elementwise multiplication. In this case the loss function is convex but the
linear predictor $\beta:=u\odot v \in \mathbb{R}^{d}$ is not in $(u, v)$.
Hence we can see from this example a rich non-convex dynamics. Then $\nabla_u
h_{u, v}(x) = v \odot x$  and the SDE model is
\begin{align}
    du_t = -\nabla_u \mathcal{L}(u_t, v_t) dt + \sqrt{\eta\delta} v_t \odot
    [X^{T}dB_t],
\end{align}
where $(B_t)_{t\ge 0}$ is the standard Brownian motion in $\mathbb{R}^{n}$
and the equations are symmetric for $(v_t)_{t\ge 0}$.

The behavior of this effective dynamics shows from (Pillaud-Vivien et al.,
2022) that the linear predictor $\beta_t = u_t \odot v_t$
\begin{enumerate}
    \item converges exponentially to zero outside of the support of $\beta^{*}$
    \item is with high probability in a $\mathcal{O}(\sqrt{\eta\delta})$
        neighborhood of $\beta^{*}$ in its support after time
        $\mathcal{O}( \delta^{-1})$.
\end{enumerate}

The first phase of SGD with large step sizes $\eta$ decreases the traning
loss until stabilization at some level set $\delta > 0$, and during this
loss-stabilization an effective dynamics takes place. Shrinking the
coordinates outside of the support of the sparsest signal and oscillates in
the parameter space at level $\mathcal{O}(\sqrt{\eta\delta})$ on its support.
Thereby decreasing the step size later leads to perfect recovery of the
sparsest predictor. TODO: Experiments.

\newline





\section{Sparse Feature Learning}

\section{Empirical Results}

\section{Comparison of the Results}

\section{Conclusion}



\nocite{andriushchenko2023sgd}
\nocite{shalev2014understanding}
\nocite{fast_armijo_2022}
\printbibliography

\end{document}
