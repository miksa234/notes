# Plan for the presentation

## Introduction
    * Setup
    * Method
    * main focus -> learning rate
    * conjecture -> large step size learns sparse features & generalizes
    better
    * but to do this we need to dive deeper in to the dynamics of an SGD
    iteration
    * SGD is GD with specific label noise

## Loss stabilization for quadratic loss
    * Setup
    * Iterates
    * Proposition for Loss staiblization
    * Proof
    * Explanation


## SGD Dynamics
    * Stochastic differential equations
    * what does it mean for sgd
    * Utilization for loss stabilizaion
    * measurement of sparse feature learning
    * feature sparsity coefficient


## Diagonal Networks
    * Setup
    * Measuring
    * Results


# SGD and GD have different implicit biases


## ReLU Networks
    * Setup
    * Measuring
    * Results

## Outlook to more comlex cases
    * Setup
    * Datasets
    * Warmup step size
    * Results

* Thats it
