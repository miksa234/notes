\include{./preamble.tex}

\begin{document}
\maketitle
\tableofcontents
\section{Sheet 6}
\subsection{Exercise 35}
Let $M \in \mathbb{R}^{n \times  n}$ with $\|M\| < 1$. Show that $I - M$ is
regular and
\begin{equation}
    \|\left( I - M \right) \| \le \frac{1}{1-\|M\|}. \label{eq: ineq}
\end{equation}
Suppose $I-M$ is not singular then for $x \in \mathbb{R}^{n}$ we have that
\begin{align}
    &\left( I - M \right)x = 0\\
    \Leftrightarrow  &Ix - Mx = 0\\
    \Leftrightarrow & Mx = x.
\end{align}
But since $\|M\|<1$ then $\forall x \in \mathbb{R}^{n}$ we have that $\|Mx\|
< \|x\|$. This means that
\begin{align}
    \text{ker}\left(I-M \right) = \emptyset,
\end{align}
so $I-M$ is regular. The identity on the other hand is derived by the
following observation
\begin{align}
    \left( I-M \right)^{-1} - \left( I-M \right)^{-1} M = \left( I -M
    \right)\left( I-M \right)^{-1} = I,
\end{align}
Then we calculate
\begin{align}
    \|\left(I-M \right)^{-1}\|
    &= \|I+\left( I-M \right)^{-1} M\|\\
    &\le \|I\| + \|(I-M)^{-1}\|\|M\|,
\end{align}
rearranging gives
\begin{align}
    \|\left( I-M \right)^{-1}\| - \|(I-M)^{-1}\| \le \|I\|\\
    \|(I-M)^{-1}\|\left(1-\|M\|  \right)  \le 1\\
    \|\left( I-M \right)^{-1}\| \le \frac{1}{1-\|M\|}.
\end{align}
Now let $A,B \in \mathbb{R}^{n \times n}$ with  $\|I-BA\|<1$. Show that $A$
and $B$ are regular and that
\begin{align}
    \|B^{-1}\| \le \frac{\|A\|}{1-\|I-BA\|}\\
    \|A^{-1}\| \le \frac{\|B\|}{1-\|I-BA\|}\\
\end{align}
We know that for $M \in \mathbb{R}^{n \times  n}$ with $\|M\|<1$ then $I-M$
is regular and the inequality in \ref{eq: ineq} holds. Set $M =
I-BA$ then $I-M=AB$ is regular. Because $AB$ is regular so is $A$ and $B$.
Now note that for all regular matrices we have that $\|A^{-1}\|\le
\|A\|^{-1}$. Furthermore
\begin{align}
    \|\left(AB  \right)^{-1}\| \le \|B^{-1}\|\|A^{-1}\|.
\end{align}
Then for $A$ we have
\begin{align}
    \|A^{-1}\|\le \frac{1}{\|B^{-1}\|}\frac{1}{1-\|I-BA\|}\le
    \frac{\|B\|}{1-\|I-BA\|}.
\end{align}
and for $B$
\begin{align}
    \|B^{-1}\|\le \frac{1}{\|A^{-1}\|}\frac{1}{1-\|I-BA\|}\le
    \frac{\|A\|}{1-\|I-BA\|}.
\end{align}
\subsection{Exercise 36}
Let $f:\mathbb{R}^{2}\to \mathbb{R}$, $f(x, y) = x^4 + 2x^2y^2 + y^4$. Show
that the local Newton algorithm converges to the unique global minimum of
$f$ for every $(x^0, y^0) \in \mathbb{R}^{2}\backslash\{(0, 0)^T\}$.
First we determine the minimum $x^*$. Note that $f(x, y) = \left( x^2 + y^2
\right)^2 \ge 0$ for all $(x, y)^T \in \mathbb{R}^{2}$. Since $f$ is strongly convex the
only minimum, which is the global minimum is $(x, y)^T = (0 ,0)^T$. The
Hessian of $f$ is
\begin{align}
    \nabla^2 f(x,y) = \begin{pmatrix}12x^2+4y^2 & 8xy\\ 8xy & 12y^2 + 4x^2
    \end{pmatrix}.
\end{align}
Now note that the Hessian at the minimum $\nabla^2f(0,0)$ is the zero matrix
which is singular. But considering starting vectors $(x, y)^T \neq (0,
0)^T$, all we need in the local Newton algorithm is the solution to the equation
$\nabla^2f(x^k)d_k = -\nabla f(x^k)$. Meaning we need to show that
$\nabla^2f(x,y)$ is regular for all $(x, y)^T \neq (0,0)^T$, in this
case we look at the determinant of the Hessian
\begin{align}
    \det(\nabla^2 f(x,y)) = 48*(x^2 + y^2)^2 > 0 \;\; \forall (x, y)^T \neq (0,
    0)^T.
\end{align}
This means that the sequence $(x^k)_{k\ge 0}$ produced by the local newton
algorithm converges to the unique global minimum of $f$ given by $(0, 0)^T$.
Indeed if we calculate the solution of the system $\nabla^2f(x,y)d =
-\nabla f(x,y)$ we get that $d = (\frac{x}{3}, \frac{y}{3})^T$.

\subsection{Exercise 37}
Show that the local Newton Algorithm is invariant to affine-linear
transformation, for a regular matrix $A \in \mathbb{R}^{n \times n}$ and $c
\in \mathbb{R}^{n}$, $(x^k)_{k\ge 0}$ the sequence generated by the local
Newton algorithm for minimizing $f$ with starting vector $x^0$. Then let
$(y^k)_{k\ge 0}$ the sequence generated by the local Newton algorithm for the
function $g(y) := f(Ay +c)$ with starting vector $y^0$, then
\begin{align}
    x^0 = Ay^0 + c \;\; \implies  x^k = Ay^k + c \;\;\; \forall k \ge 0.
\end{align}
First of all we calculate the gradient and the hessian for $g$
\begin{align}
    \nabla g(y) = \nabla f(Ay+c) = A^T \nabla f(Ay+c)\\
    \nabla^2 g(y) = \nabla^2 f(Ay+c) = A^T \nabla^2 f(Ay+c) A\\
\end{align}
Now we need to prove that $x^{k+1} = Ay^{k+1} + c$
\begin{align}
    x^{k+1}q
    &= x^{k} + d_k = x^{k} - \left( \nabla^2 f(x^{k})\right)^{-1}
    \nabla f(x_k)\\
    &=Ay^k + c - \left( \nabla^2(f(Ay^{k}+c )\right)^{-1} \nabla f(Ay^{k}+c)\\
    &=Ay^{k} + c - A A^{-1}\left( \nabla^2(f(Ay^{k}+c )\right)^{-1} \nabla f(Ay^{k}+c)\\
    &=Ay^{k} + c - A A^{-1} A^T A^{-T}\left( \nabla^2(f(Ay^{k}+c )\right)^{-1} \nabla f(Ay^{k}+c)\\
    &=A\left(y^{k} - \left( A^{T}\nabla^2 f(Ay^{k}+c)A\right)^{-1}A^T \nabla
    f(Ay^{k}+c ) \right)  +c \\
    &= Ay^{k+1} +c.
\end{align}
by that the induction is finished.
\subsection{Exercise 38}
Let $M \in \mathbb{R}^{n \times n}$ be a regular matrix and $\{M\}_{k\ge 0}
\in \mathbb{R}^{n \times n}$ a sequence of matrices which converge to M as $k
\to \infty $. Sow that there exists a $k_0 \ge 0$ such that $M_k$ is regular
for all $k\ge k_0$ and the sequence $\{M_k^{-1}\}_{k\ge 0}$ converges to
$M^{-1}$.
\newline
The map $M \to M^{-1}$ is a continuous invertible meaning it is monotone.
$M^{-1} = \frac{\text{adj}(M)}{\text{det}(M)}$. Then convergence means that
there is a $k\ge k_0$ such that for all $M_k \in B_{\frac{1}{k}}(M)$ we have
that $\|M_k -M\| < \frac{1}{k}$ then $M_k$ is sufficiently close to $M$ and
so regular. Since $\{M_k\}_{k\ge k_0} \cup {M}$ is a compact set
of invertible matrices so is $\{M_k^{-1}\}_{k\ge k_0} \cup {M^{-1}}$,
meaning it is bounded. This means that $\{M^{-1}_k\}_{k\ge k_0}$ converges to
$M^{-1}$.
\subsection{Exercise 39}
Let $H \in \mathbb{R}^{n \times n}$ be regular $u, v \in \mathbb{R}^{n}$
arbitrary. Show that $H+uv^T$ regular $\Leftrightarrow$  $1+v^TH^{-1}u \neq
0$, then the Sherman-Morrison formula holds
\begin{align}
    \left( H + uv^T \right)^{-1} = \left( I - \frac{1}{1-v^TH^{-1} u} H^{-1}
    uv^T\right)H^{-1} \label{eq: smf}
\end{align}
Let $1+ v^TH^{-1}u =0$ then
\begin{align}
    \text{det}\left( H + uv^T \right) =(1+v^T H^{-1} u)\text{det}(H) = 0.
\end{align}
This means that $H$ is not invertible. Now we need to check if the inverse
really holds which is done by simply multiplying
\begin{align}
    &\left( H+uv^{T} \right) \left( H^{-1} - \frac{H^{-1} uv^T
    H^{-1}}{1+v^{T}H^{-1}u} \right)  = \\
    &=H H^{-1} + uv^T H^{-1}  - H \frac{H^{-1}uv^T H^{-1}}{1+v^TH^{-1}u}
    uv^{T}\frac{H^{-1}uv^TH^{-1}}{1+v^{T}H^{-1}u}\\
    &=I + uv^T H^{-1} - \frac{uv^T H^{-1} + uv^T H^{-1}uv^T
    H^{-1}}{1+v^TH^{-1}u}\\
    &=I+uv^TH^{-1} - \frac{u\left( 1+v^{T}H^{-1}u \right)
    v^{T}H^{-1}}{1+v^{T}H^{-1} u}\\
    &=I + uv^{T}H^{-1} - uv^{T}H^{-1} \\
    &= I
\end{align}
Since these are square matrices $AB=I$ is the same as $BA=I$.
\subsection{Exercise 40}
Consider the quadratic optimization problem
\begin{align}
    \min \quad &f(x):=\gamma + c^{T}x + \frac{1}{2}x^{T}Qx, \label{eq: opp}\\
    \text{s.t}\quad &h(x) := b^{T}x = 0, \nonumber
\end{align}
with $Q \in \mathbb{R}^{n \times n}$ SPD , $b, c \in \mathbb{R}^{n}$, $b\neq
0$ and $\gamma \in \mathbb{R}$. For a given $\alpha > 0$ find the minimum
$x^*(\alpha)$ of the penalty function
\begin{align}
    P(x;\alpha) := f(x) + \frac{\alpha}{2}\left( h(x) \right)^{2}
\end{align}
determine $x^* := \lim_{\alpha \to \infty}x^{*}(\alpha)$ and prove that
$x^{*}$ is a unique optimal solution of the optimization problem in \ref{eq:
opp}. We start with calculating the minimum of $P(x(\alpha))$
\begin{align}
    \nabla P(x(\alpha))
    &= \nabla f(x) + \frac{\alpha}{2}\nabla h(x)^{2}\\
    &= c + Qx + \frac{\alpha}{2} 2 h(x) \nabla h(x) \\
    &=c  + Qx + \alpha b^T x b = 0\\
    \nonumber\\
    Qx + \alpha b b ^T x = -c \\
    \left( Q + \alpha b b^T \right) x = -c.
\end{align}
Using the Sherman-Morrison formula in \ref{eq: smf} for $H = Q$, $u=\alpha
b$ and $v = b$ we get
\begin{align}
    x^{*}(\alpha) = \left( \frac{\alpha}{1+\alpha b^{T}Q^{-1}b}Q^{-1} bb^{T}
    -I\right) Q^{-1} c
\end{align}
The limit is then (the standard limit $\frac{x}{1+kx} \to \frac{1}{k}$ as $x$
goes to infinity)
\begin{align}
    x^{*} &= \lim_{\alpha \to \infty}x^{*}(\alpha)\\
          &= \left(\frac{Q^{-1}bb^{T}}{b^{T}Q^{-1}b} -I \right) Q^{-1} c.
\end{align}
To show that $x^{*}$ is a unique solution of the optimization problem
\ref{eq: opp} we need to show that it satisfies the optimality condition and
that it is unique. Now it is unique because $Q$ is SPD meaning it is regular
and invertible and $\nabla^2 f = Q>0$. Further more $(x^{*}, \alpha)$ is a
KKT point of $P(x, \alpha)$ then $x^{*}$ is a minumum of the optimization
problem. Now we show that $b^{T}x^{*} = 0$:
\begin{align}
    b^{T}x^{*} &= \left(\frac{b^{T}Q^{-1}bb^{T}Q^{-1}}{b^{T}Q^{-1}b}-
    b^{T}Q^{-1} \right) c \\
    &= \left( b^{T}Q - b^{T}Q \right) c \\
    &= 0.
\end{align}
\end{document}


